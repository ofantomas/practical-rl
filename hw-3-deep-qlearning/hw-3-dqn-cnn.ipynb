{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network CNN implementation (up to 10 points)\n",
    "\n",
    "This homework demands you to implement a DQN - an approximate q-learning algorithm with experience replay and target networks - and see if it works any better this way.\n",
    "\n",
    "Original paper:\n",
    "https://arxiv.org/pdf/1312.5602.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook is the main notebook.** Another notebook is given for the first view (**hw-3-dqn-mlp**). The tasks are similar and share most of the code. The main difference is in environments. In main notebook it can take some 2 hours for the agent to start improving so it seems reasonable to launch the algorithm on a simpler env first. Here it is CartPole and it will train in several minutes.\n",
    "\n",
    "**We suggest the following pipeline:** First implement the mlp algorithm then implement the cnn one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-08-28T07:23:08.522333Z",
     "start_time": "2020-08-28T07:23:07.301031Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play some old videogames. This time we're gonna apply approximate q-learning to an atari game called Breakout. It's not the hardest thing out there, but it's definitely way more complex than anything we tried before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME = \"BreakoutNoFrameskip-v4\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what observations look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(ENV_NAME)\n",
    "env.reset()\n",
    "\n",
    "n_cols = 5\n",
    "n_rows = 2\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, row * n_cols + col + 1)\n",
    "        ax.imshow(env.render('rgb_array'))\n",
    "        env.step(env.action_space.sample())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's play a little.**\n",
    "\n",
    "Pay attention to zoom and fps args of play function. Control: A, D, space."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# This block is just for fun and work only on your local pc\n",
    "# Make keyboard interrupt to continue\n",
    "\n",
    "from gym.utils.play import play\n",
    "\n",
    "play(env=gym.make(ENV_NAME), zoom=4, fps=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing game image \n",
    "\n",
    "Raw atari images are large, 210x160x3 by default. However, we don't need that level of detail in order to learn them.\n",
    "\n",
    "We can thus save a lot of time by preprocessing game image, including\n",
    "* Resizing to a smaller shape, 64 x 64\n",
    "* Converting to grayscale\n",
    "* Cropping irrelevant image parts (top, bottom and edges)\n",
    "\n",
    "Also please keep one dimension for channel so that final shape would be 1 x 64 x 64.\n",
    "\n",
    "Tip: You can implement your own grayscale converter and assign a huge weight to the red channel. This trick is not necessary but it will speed up learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.core import ObservationWrapper\n",
    "from gym.spaces import Box\n",
    "\n",
    "\n",
    "class PreprocessAtariObs(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"A gym wrapper that crops, scales image into the desired shapes and grayscales it.\"\"\"\n",
    "        ObservationWrapper.__init__(self, env)\n",
    "\n",
    "        self.img_size = (1, 64, 64)\n",
    "        self.observation_space = Box(0.0, 1.0, self.img_size)\n",
    "\n",
    "    def _to_gray_scale(self, rgb, channel_weights=[0.8, 0.1, 0.1]):\n",
    "        return (rgb * np.array(channel_weights).reshape(1, 1, -1)).sum(2)\n",
    "    \n",
    "    def _crop_image(self, img):\n",
    "        #hardcoded indices\n",
    "        return img[30:195, 6:-6]\n",
    "\n",
    "    def observation(self, img):\n",
    "        \"\"\"what happens to each observation\"\"\"\n",
    "        obs = self._to_gray_scale(img)\n",
    "        obs = self._crop_image(obs)\n",
    "        obs = cv2.resize(obs, dsize = (64, 64))[np.newaxis, ...]\n",
    "        return (obs / 256).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formal tests seem fine. Here's an example of what you'll get.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spawn game instance for tests\n",
    "env = gym.make(ENV_NAME)  # create raw env\n",
    "env = PreprocessAtariObs(env)\n",
    "observation_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "env.reset()\n",
    "obs, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "# test observation\n",
    "assert obs.ndim == 3, \"observation must be [channel, h, w] even if there's just one channel\"\n",
    "assert obs.shape == observation_shape\n",
    "assert obs.dtype == 'float32'\n",
    "assert len(np.unique(obs)) > 2, \"your image must not be binary\"\n",
    "assert 0 <= np.min(obs) and np.max(\n",
    "    obs) <= 1, \"convert image pixels to [0,1] range\"\n",
    "\n",
    "print(\"Formal tests seem fine. Here's an example of what you'll get.\")\n",
    "\n",
    "n_cols = 5\n",
    "n_rows = 2\n",
    "fig = plt.figure(figsize=(16, 9))\n",
    "obs = env.reset()\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        ax = fig.add_subplot(n_rows, n_cols, row * n_cols + col + 1)\n",
    "        ax.imshow(obs[0, :, :], interpolation='none', cmap='gray')\n",
    "        obs, _, _, _ = env.step(env.action_space.sample())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the game:** You have 5 lives and get points for breaking the wall. Higher bricks cost more than the lower ones. There are 4 actions: start game (should be called at the beginning and after each life is lost), move left, move right and do nothing. There are some common wrappers used for Atari environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import atari_wrappers\n",
    "\n",
    "def PrimaryAtariWrap(env, clip_rewards=True):\n",
    "    assert 'NoFrameskip' in env.spec.id\n",
    "\n",
    "    # This wrapper holds the same action for <skip> frames and outputs\n",
    "    # the maximal pixel value of 2 last frames (to handle blinking\n",
    "    # in some envs)\n",
    "    env = atari_wrappers.MaxAndSkipEnv(env, skip=4)\n",
    "\n",
    "    # This wrapper sends done=True when each life is lost\n",
    "    # (not all the 5 lives that are givern by the game rules).\n",
    "    # It should make easier for the agent to understand that losing is bad.\n",
    "    env = atari_wrappers.EpisodicLifeEnv(env)\n",
    "\n",
    "    # This wrapper laucnhes the ball when an episode starts.\n",
    "    # Without it the agent has to learn this action, too.\n",
    "    # Actually it can but learning would take longer.\n",
    "    env = atari_wrappers.FireResetEnv(env)\n",
    "\n",
    "    # This wrapper transforms rewards to {-1, 0, 1} according to their sign\n",
    "    if clip_rewards:\n",
    "        env = atari_wrappers.ClipRewardEnv(env)\n",
    "\n",
    "    # This wrapper is yours :)\n",
    "    env = PreprocessAtariObs(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's see if the game is still playable after applying the wrappers.**\n",
    "At playing the EpisodicLifeEnv wrapper seems not to work but actually it does (because after when life finishes a new ball is dropped automatically - it means that FireResetEnv wrapper understands that a new episode began)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not work in colab.\n",
    "# make keyboard interrupt to continue\n",
    "\n",
    "from gym.utils.play import play\n",
    "\n",
    "def make_play_env():\n",
    "    env = gym.make(ENV_NAME)\n",
    "    env = PrimaryAtariWrap(env)\n",
    "# in torch imgs have shape [c, h, w] instead of common [h, w, c]\n",
    "    env = atari_wrappers.AntiTorchWrapper(env)\n",
    "    return env\n",
    "\n",
    "play(make_play_env(), zoom=4, fps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frame Buffer\n",
    "\n",
    "Our agent can only process one observation at a time, so we gotta make sure it contains enough information to find optimal actions. For instance, agent has to react to moving objects so he must be able to measure object's velocity. To do so, we introduce a buffer that stores 4 last images. This time everything is pre-implemented for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from framebuffer import FrameBuffer\n",
    "\n",
    "def make_env(clip_rewards=True, seed=None):\n",
    "    env = gym.make(ENV_NAME)  # create raw env\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "    env = PrimaryAtariWrap(env, clip_rewards)\n",
    "    env = FrameBuffer(env, n_frames=4, dim_order='pytorch')\n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions, state_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(12):\n",
    "    obs, _, _, _ = env.step(env.action_space.sample())\n",
    "\n",
    "plt.figure(figsize=[12,10])\n",
    "plt.title(\"Game image\")\n",
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=[15,15])\n",
    "plt.title(\"Agent observation (4 frames top to bottom)\")\n",
    "plt.imshow(utils.img_by_obs(obs, state_shape), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN as it is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a network\n",
    "\n",
    "We now need to build a neural network that can map images to state q-values. This network will be called on every agent's step so it better not be resnet-152 unless you have an array of GPUs. Instead, you can use strided convolutions with a small number of features to save time and memory.\n",
    "\n",
    "You can build any architecture you want, but for reference, here's something that will more or less work:\n",
    "\n",
    "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/dqn_arch.png width=640>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# those who have a GPU but feel unfair to use it can uncomment:\n",
    "# device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_size_out(size, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    common use case:\n",
    "    cur_layer_img_w = conv2d_size_out(cur_layer_img_w, kernel_size, stride)\n",
    "    cur_layer_img_h = conv2d_size_out(cur_layer_img_h, kernel_size, stride)\n",
    "    to understand the shape for dense layer's input\n",
    "    \"\"\"\n",
    "    return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_shape, n_actions, epsilon=0):\n",
    "\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "\n",
    "        # Define your network body here. Please make sure agent is fully contained here\n",
    "        <YOUR CODE>\n",
    "\n",
    "    def forward(self, state_t):\n",
    "        \"\"\"\n",
    "        takes agent's observation (tensor), returns qvalues (tensor)\n",
    "        :param state_t: a batch of 4-frame buffers, shape = [batch_size, 4, h, w]\n",
    "        \"\"\"\n",
    "        # Use your network to compute qvalues for given state\n",
    "        qvalues = <YOUR CODE>\n",
    "\n",
    "        assert len(\n",
    "            qvalues.shape) == 2 and qvalues.shape[0] == state_t.shape[0] and qvalues.shape[1] == n_actions\n",
    "\n",
    "        return qvalues\n",
    "\n",
    "    def get_qvalues(self, states):\n",
    "        \"\"\"\n",
    "        like forward, but works on numpy arrays, not tensors\n",
    "        \"\"\"\n",
    "        model_device = next(self.parameters()).device\n",
    "        states = torch.tensor(states, device=model_device, dtype=torch.float)\n",
    "        qvalues = self.forward(states)\n",
    "        return qvalues.data.cpu().numpy()\n",
    "\n",
    "    def sample_actions(self, qvalues):\n",
    "        \"\"\"pick actions given qvalues. Uses epsilon-greedy exploration strategy. \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        batch_size, n_actions = qvalues.shape\n",
    "\n",
    "        random_actions = np.random.choice(n_actions, size=batch_size)\n",
    "        best_actions = qvalues.argmax(axis=-1)\n",
    "\n",
    "        should_explore = np.random.choice(\n",
    "            [0, 1], batch_size, p=[1-epsilon, epsilon])\n",
    "        return np.where(should_explore, random_actions, best_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(state_shape, n_actions, epsilon=0.5).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try out our agent to see if it raises any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=False, t_max=10000):\n",
    "    \"\"\" Plays n_games full games. If greedy, picks actions as argmax(qvalues). Returns mean reward. \"\"\"\n",
    "    rewards = []\n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            qvalues = agent.get_qvalues([s])\n",
    "            action = qvalues.argmax(axis=-1)[0] if greedy else agent.sample_actions(qvalues)[0]\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        rewards.append(reward)\n",
    "    return np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(env, agent, n_games=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay\n",
    "There's a powerful technique that you can use to improve sample efficiency for off-policy algorithms: Experience Replay. The catch is that you can train Q-learning and EV-SARSA on `<s,a,r,s'>` tuples even if they aren't sampled under current agent's policy. So here's what we're gonna do:\n",
    "\n",
    "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/exp_replay.png width=640>\n",
    "\n",
    "#### Training with experience replay\n",
    "1. Play game, sample `<s,a,r,s'>`.\n",
    "2. Update q-values based on `<s,a,r,s'>`.\n",
    "3. Store `<s,a,r,s'>` transition in a buffer. \n",
    " 3. If buffer is full, delete earliest data.\n",
    "4. Sample K such transitions from that buffer and update q-values based on them.\n",
    "\n",
    "#### The interface is fairly simple:\n",
    "* `exp_replay.add(obs, act, rw, next_obs, done)` - saves (s,a,r,s',done) tuple into the buffer\n",
    "* `exp_replay.sample(batch_size)` - returns observations, actions, rewards, next_observations and is_done for `batch_size` random samples.\n",
    "* `len(exp_replay)` - returns number of elements stored in replay buffer.\n",
    "\n",
    "\n",
    "To enable such training, first we must implement a memory structure that would act like such a buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, size):\n",
    "        \"\"\"\n",
    "        Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "\n",
    "        Note: for this assignment you can pick any data structure you want.\n",
    "              If you want to keep it simple, you can store a list of tuples of (s, a, r, s') in self._storage\n",
    "              However you may find out there are faster and/or more memory-efficient ways to do so.\n",
    "        \"\"\"\n",
    "        self._storage = []\n",
    "        self._maxsize = size\n",
    "        self._index = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._storage)\n",
    "\n",
    "    def add(self, obs_t, action, reward, obs_tp1, done):\n",
    "        '''\n",
    "        Make sure, _storage will not exceed _maxsize. \n",
    "        Make sure, FIFO rule is being followed: the oldest examples has to be removed earlier\n",
    "        '''\n",
    "        data = (obs_t, action, reward, obs_tp1, done)\n",
    "        maxsize = self._maxsize\n",
    "        \n",
    "        if len(self._storage) < maxsize:\n",
    "            self._storage.append(data)\n",
    "        else:\n",
    "            self._storage[self._index] = data\n",
    "            self._index = (self._index + 1) % self._maxsize\n",
    "        \n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        obs_batch: np.array\n",
    "            batch of observations\n",
    "        act_batch: np.array\n",
    "            batch of actions executed given obs_batch\n",
    "        rew_batch: np.array\n",
    "            rewards received as results of executing act_batch\n",
    "        next_obs_batch: np.array\n",
    "            next set of observations seen after executing act_batch\n",
    "        done_mask: np.array\n",
    "            done_mask[i] = 1 if executing act_batch[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        storage = self._storage\n",
    "        batch = random.sample(storage, batch_size)\n",
    "        state, actions, rewards, next_states, is_done = list(map(np.array, (zip(*batch))))\n",
    "        \n",
    "        return state, actions, rewards, next_states, is_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_replay = ReplayBuffer(10)\n",
    "\n",
    "for _ in range(30):\n",
    "    exp_replay.add(env.reset(), env.action_space.sample(),\n",
    "                   1.0, env.reset(), done=False)\n",
    "\n",
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "    5)\n",
    "\n",
    "assert len(exp_replay) == 10, \"experience replay size should be 10 because that's what maximum capacity is\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_record(initial_state, agent, env, exp_replay, n_steps=1):\n",
    "    \"\"\"\n",
    "    Play the game for exactly n steps, record every (s,a,r,s', done) to replay buffer. \n",
    "    Whenever game ends, add record with done=True and reset the game.\n",
    "    It is guaranteed that env has done=False when passed to this function.\n",
    "\n",
    "    PLEASE DO NOT RESET ENV UNLESS IT IS \"DONE\"\n",
    "\n",
    "    :returns: return sum of rewards over time and the state in which the env stays\n",
    "    \"\"\"\n",
    "    s = initial_state\n",
    "    sum_rewards = 0\n",
    "\n",
    "    # Play the game for n_steps as per instructions above\n",
    "    for step in range(n_steps):\n",
    "        qvalues = agent.get_qvalues([s])\n",
    "        action = agent.sample_actions(qvalues)[0]\n",
    "        next_s, r, done, _ = env.step(action)\n",
    "        sum_rewards += r\n",
    "        exp_replay.add(s, action, r, next_s, done)\n",
    "        if done is True:\n",
    "            s = env.reset()\n",
    "        else:\n",
    "            s = next_s\n",
    "\n",
    "    return sum_rewards, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing your code.\n",
    "exp_replay = ReplayBuffer(2000)\n",
    "\n",
    "state = env.reset()\n",
    "play_and_record(state, agent, env, exp_replay, n_steps=1000)\n",
    "\n",
    "# if you're using your own experience replay buffer, some of those tests may need correction.\n",
    "# just make sure you know what your code does\n",
    "assert len(exp_replay) == 1000, \"play_and_record should have added exactly 1000 steps, \"\\\n",
    "                                 \"but instead added %i\" % len(exp_replay)\n",
    "is_dones = list(zip(*exp_replay._storage))[-1]\n",
    "\n",
    "assert 0 < np.mean(is_dones) < 0.1, \"Please make sure you restart the game whenever it is 'done' and record the is_done correctly into the buffer.\"\\\n",
    "                                    \"Got %f is_done rate over %i steps. [If you think it's your tough luck, just re-run the test]\" % (\n",
    "                                        np.mean(is_dones), len(exp_replay))\n",
    "\n",
    "for _ in range(100):\n",
    "    obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "        10)\n",
    "    assert obs_batch.shape == next_obs_batch.shape == (10,) + state_shape\n",
    "    assert act_batch.shape == (\n",
    "        10,), \"actions batch should have shape (10,) but is instead %s\" % str(act_batch.shape)\n",
    "    assert reward_batch.shape == (\n",
    "        10,), \"rewards batch should have shape (10,) but is instead %s\" % str(reward_batch.shape)\n",
    "    assert is_done_batch.shape == (\n",
    "        10,), \"is_done batch should have shape (10,) but is instead %s\" % str(is_done_batch.shape)\n",
    "    assert [int(i) in (0, 1)\n",
    "            for i in is_dones], \"is_done should be strictly True or False\"\n",
    "    assert [\n",
    "        0 <= a < n_actions for a in act_batch], \"actions should be within [0, n_actions)\"\n",
    "\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target networks\n",
    "\n",
    "We also employ the so called \"target network\" - a copy of neural network weights to be used for reference Q-values:\n",
    "\n",
    "The network itself is an exact copy of agent network, but it's parameters are not trained. Instead, they are moved here from agent's actual network every so often.\n",
    "\n",
    "$$ Q_{reference}(s,a) = r + \\gamma \\cdot \\max _{a'} Q_{target}(s',a') $$\n",
    "\n",
    "\n",
    "<img src=https://github.com/yandexdataschool/Practical_RL/raw/master/yet_another_week/_resource/target_net.png width=640>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_network = DQNAgent(agent.state_shape, agent.n_actions, epsilon=0.5).to(device)\n",
    "# This is how you can load weights from agent into target network\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with... Q-learning\n",
    "Here we write a function similar to `agent.update` from tabular q-learning.\n",
    "\n",
    "Compute Q-learning TD error:\n",
    "\n",
    "$$ L = { 1 \\over N} \\sum_i [ Q_{\\theta}(s,a) - Q_{reference}(s,a) ] ^2 $$\n",
    "\n",
    "With Q-reference defined as\n",
    "\n",
    "$$ Q_{reference}(s,a) = r(s,a) + \\gamma \\cdot \\max_{a'} Q_{target}(s', a') $$\n",
    "\n",
    "Where\n",
    "* $Q_{target}(s',a')$ denotes q-value of next state and next action predicted by __target_network__\n",
    "* $s, a, r, s'$ are current state, action, reward and next state respectively\n",
    "* $\\gamma$ is a discount factor defined two cells above.\n",
    "\n",
    "\n",
    "__Note 1:__ there's an example input below. Feel free to experiment with it before you write the function.\n",
    "\n",
    "__Note 2:__ compute_td_loss is a source of 99% of bugs in this homework. If reward doesn't improve, it often helps to go through it line by line [with a rubber duck](https://rubberduckdebugging.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                    agent, target_network,\n",
    "                    gamma=0.99,\n",
    "                    check_shapes=False,\n",
    "                    device=device):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formulae above. \"\"\"\n",
    "    states = torch.tensor(states, device=device, dtype=torch.float)    # shape: [batch_size, *state_shape]\n",
    "    # for some torch reason should not make actions a tensor\n",
    "    actions = torch.tensor(actions, device=device, dtype=torch.long)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, device=device, dtype=torch.float)  # shape: [batch_size]\n",
    "    next_states = torch.tensor(next_states, device=device, dtype=torch.float) # shape: [batch_size, *state_shape]\n",
    "    is_done = torch.tensor(is_done.astype('float32'), device=device, dtype=torch.float)  # shape: [batch_size]\n",
    "    is_not_done = 1 - is_done\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = agent(states)\n",
    "    assert predicted_qvalues.requires_grad, \"qvalues must be a torch tensor with grad\"\n",
    "    \n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[range(len(actions)), actions]\n",
    "\n",
    "    # compute q-values for all actions in next states\n",
    "    # and use it to compute V*(next_states) \n",
    "    with torch.no_grad():\n",
    "        predicted_next_qvalues = target_network(next_states)\n",
    "        next_state_values = predicted_next_qvalues.max(1)[0] * is_not_done\n",
    "\n",
    "    assert next_state_values.dim(\n",
    "    ) == 1 and next_state_values.shape[0] == states.shape[0], \"must predict one value per state\"\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    # at the last state use the simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    # you can multiply next state values by is_not_done to achieve this.\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values\n",
    "    \n",
    "    assert target_qvalues_for_actions.requires_grad == False, \"do not send gradients to target!\"\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions -\n",
    "                       target_qvalues_for_actions) ** 2)\n",
    "\n",
    "    if check_shapes:\n",
    "        assert predicted_next_qvalues.data.dim(\n",
    "        ) == 2, \"make sure you predicted q-values for all actions in next state\"\n",
    "        assert next_state_values.data.dim(\n",
    "        ) == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "        assert target_qvalues_for_actions.data.dim(\n",
    "        ) == 1, \"there's something wrong with target q-values, they must be a vector\"\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch = exp_replay.sample(\n",
    "    10)\n",
    "\n",
    "loss = compute_td_loss(obs_batch, act_batch, reward_batch, next_obs_batch, is_done_batch,\n",
    "                       agent, target_network,\n",
    "                       gamma=0.99, check_shapes=True)\n",
    "loss.backward()\n",
    "\n",
    "assert loss.requires_grad and tuple(loss.data.size()) == (\n",
    "    ), \"you must return scalar loss - mean over batch\"\n",
    "assert np.any(next(agent.parameters()).grad.data.cpu().numpy() !=\n",
    "              0), \"loss must be differentiable w.r.t. network weights\"\n",
    "assert np.all(next(target_network.parameters()).grad is None), \"target network should not have grads\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main loop\n",
    "\n",
    "It's time to put everything together and see if it learns anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = <your favourite random seed>\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_env(seed)\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n\n",
    "state = env.reset()\n",
    "\n",
    "agent = DQNAgent(state_shape, n_actions, epsilon=1).to(device)\n",
    "target_network = DQNAgent(state_shape, n_actions).to(device)\n",
    "target_network.load_state_dict(agent.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buffer of size $10^4$ fits into 5 Gb RAM.\n",
    "\n",
    "Larger sizes ($10^5$ and $10^6$ are common) can be used. It can improve the learning, but $10^4$ is quiet enough. $10^2$ will probably fail learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_replay = ReplayBuffer(10**4)\n",
    "for i in range(100):\n",
    "    if not utils.is_enough_ram(min_available_gb=0.1):\n",
    "        print(\"\"\"\n",
    "            Less than 100 Mb RAM available. \n",
    "            Make sure the buffer size in not too huge.\n",
    "            Also check, maybe other processes consume RAM heavily.\n",
    "            \"\"\"\n",
    "             )\n",
    "        break\n",
    "    play_and_record(state, agent, env, exp_replay, n_steps=10**2)\n",
    "    if len(exp_replay) == 10**4:\n",
    "        break\n",
    "print(len(exp_replay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps_per_epoch = 1\n",
    "batch_size = 32\n",
    "total_steps = 3 * 10**6\n",
    "decay_steps = 10**6\n",
    "\n",
    "opt = torch.optim.Adam(agent.parameters(), lr=1e-4)\n",
    "\n",
    "init_epsilon = 1\n",
    "final_epsilon = 0.1\n",
    "\n",
    "loss_freq = 50\n",
    "refresh_target_network_freq = 5000\n",
    "eval_freq = 5000\n",
    "\n",
    "max_grad_norm = 50\n",
    "\n",
    "n_lives = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "grad_norm_history = []\n",
    "initial_state_v_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "for step in trange(total_steps + 1):\n",
    "    if not utils.is_enough_ram():\n",
    "        print('less that 100 Mb RAM available, freezing')\n",
    "        print('make sure everything is ok and make KeyboardInterrupt to continue')\n",
    "        try:\n",
    "            while True:\n",
    "                pass\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "    agent.epsilon = utils.linear_decay(init_epsilon, final_epsilon, step, decay_steps)\n",
    "\n",
    "    # play\n",
    "    _, state = play_and_record(state, agent, env, exp_replay, timesteps_per_epoch)\n",
    "\n",
    "    # train\n",
    "    states, actions, rewards, next_states, is_done = exp_replay.sample(batch_size)\n",
    "    loss = compute_td_loss(states, actions, rewards, next_states, is_done,\n",
    "                           agent, target_network)\n",
    "\n",
    "\n",
    "    loss.backward()\n",
    "    grad_norm = nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    if step % loss_freq == 0:\n",
    "        td_loss_history.append(loss.data.cpu().item())\n",
    "        grad_norm_history.append(grad_norm)\n",
    "\n",
    "    if step % refresh_target_network_freq == 0:\n",
    "        # Load agent weights into target_network\n",
    "        #target_network.parameters() = agent.parameters()\n",
    "        target_network.load_state_dict(agent.state_dict())\n",
    "\n",
    "    if step % eval_freq == 0:\n",
    "        # eval the agent\n",
    "        mean_rw_history.append(evaluate(\n",
    "            make_env(seed=step), agent, n_games=3, greedy=True, t_max=1000)\n",
    "        )\n",
    "        initial_state_q_values = agent.get_qvalues(\n",
    "            [make_env(seed=step).reset()]\n",
    "        )\n",
    "        initial_state_v_history.append(np.max(initial_state_q_values))\n",
    "\n",
    "        clear_output(True)\n",
    "        print(\"buffer size = %i, epsilon = %.5f\" %\n",
    "              (len(exp_replay), agent.epsilon))\n",
    "\n",
    "        plt.figure(figsize=[16, 9])\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.title(\"Mean reward per episode\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        assert not np.isnan(td_loss_history[-1])\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.title(\"TD loss history (smoothened)\")\n",
    "        plt.plot(utils.smoothen(td_loss_history))\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.title(\"Initial state V\")\n",
    "        plt.plot(initial_state_v_history)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.title(\"Grad norm history (smoothened)\")\n",
    "        plt.plot(utils.smoothen(grad_norm_history))\n",
    "        plt.grid()\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent is evaluated for 1 life, not for a whole episode of 5 lives. Rewards in evaluation are also truncated. Cuz this is what environment the agent is learning in and in this way mean rewards per life can be compared with initial state value\n",
    "\n",
    "The goal is to get 10 points in the real env. So 2 or better 3 points in the preprocessed one will probably be enough. You can interrupt learning then."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final scoring is done on a whole episode with all 5 lives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_score = evaluate(\n",
    "  make_env(clip_rewards=False, seed=9),\n",
    "    agent, n_games=30, greedy=True, t_max=10 * 1000\n",
    ") * n_lives\n",
    "print('final score:', final_score)\n",
    "assert final_score > 10, 'not as cool as DQN can'\n",
    "print('Cool!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to interpret plots:\n",
    "\n",
    "This aint no supervised learning so don't expect anything to improve monotonously. \n",
    "* **TD loss** is the MSE between agent's current Q-values and target Q-values. It may slowly increase or decrease, it's ok. The \"not ok\" behavior includes going NaN or stayng at exactly zero before agent has perfect performance.\n",
    "* **grad norm** just shows the intensivity of training. Not ok is growing to values of about 100 (or maybe even 50) though it depends on network architecture.\n",
    "* **mean reward** is the expected sum of r(s,a) agent gets over the full game session. It will oscillate, but on average it should get higher over time (after a few thousand iterations...). \n",
    " * In basic q-learning implementation it takes about 40k steps to \"warm up\" agent before it starts to get better.\n",
    "* **Initial state V** is the expected discounted reward for episode in the oppinion of the agent. It should behave more smoothly than **mean reward**. It should get higher over time but sometimes can experience drawdowns because of the agent's overestimates.\n",
    "* **buffer size** - this one is simple. It should go up and cap at max size.\n",
    "* **epsilon** - agent's willingness to explore. If you see that agent's already at 0.01 epsilon before it's average reward is above 0 - it means you need to increase epsilon. Set it back to some 0.2 - 0.5 and decrease the pace at which it goes down.\n",
    "* Smoothing of plots is done with a gaussian kernel\n",
    "\n",
    "At first your agent will lose quickly. Then it will learn to suck less and at least hit the ball a few times before it loses. Finally it will learn to actually score points.\n",
    "\n",
    "**Training will take time.** A lot of it actually. Probably you will not see any improvment during first **150k** time steps (note that by default in this notebook agent is evaluated every 5000 time steps).\n",
    "\n",
    "## About hyperparameters:\n",
    "\n",
    "The task has something in common with supervised learning: loss is optimized through the buffer (instead of Train dataset). But the distribution of states and actions in the buffer **is not stationary** and depends on the policy that generated it. It can even happen that the mean TD error across the buffer is very low but the performance is extremely poor (imagine the agent collecting data to the buffer always manages to avoid the ball).\n",
    "\n",
    "* Total timesteps and training time: It seems to be so huge, but actually it is normal for RL.\n",
    "\n",
    "* $\\epsilon$ decay shedule was taken from the original paper and is like traditional for epsilon-greedy policies. At the beginning of the training the agent's greedy policy is poor so many random actions should be taken.\n",
    "\n",
    "* Optimizer: In the original paper RMSProp was used (they did not have Adam in 2013) and it can work not worse than Adam. For us Adam was default and it worked.\n",
    "\n",
    "* lr: $10^{-3}$ would probably be too huge\n",
    "\n",
    "* batch size: This one can be very important: if it is too small the agent can fail to learn. Huge batch takes more time to process. If batch of size 8 can not be processed on the hardware you use take 2 (or even 4) batches of size 4, divide the loss on them by 2 (or 4) and make optimization step after both backward() calls in torch.\n",
    "\n",
    "* target network update frequency: has something in common with learning rate. Too frequent updates can lead to divergence. Too rare can lead to slow leraning. For millions of total timesteps thousands of inner steps seem ok. One iteration of target network updating is an iteration of the (this time approximate) $\\gamma$-compression that stands behind Q-learning. The more inner steps it makes the more accurate is the compression.\n",
    "* max_grad_norm - just huge enough. In torch clip_grad_norm also evaluates the norm before clipping and it can be convenient for logging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# record sessions\n",
    "import gym.wrappers\n",
    "env_monitor = gym.wrappers.Monitor(make_env(), directory=\"videos\", force=True)\n",
    "sessions = [evaluate(env_monitor, agent, n_games=n_lives, greedy=True) for _ in range(10)]\n",
    "env_monitor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's have a closer look at this (up to 5 points)\n",
    "\n",
    "If average episode score is below 200 using all 5 lives, then probably DQN has not converged fully. But anyway let's make a more complete record of an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = make_env(clip_rewards=False)\n",
    "record = utils.play_and_log_episode(eval_env, agent)\n",
    "print('total reward for life:', np.sum(record['rewards']))\n",
    "for key in record:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "ax.scatter(record['v_mc'], record['v_agent'])\n",
    "ax.plot(sorted(record['v_mc']), sorted(record['v_mc']),\n",
    "       'black', linestyle='--', label='x=y')\n",
    "\n",
    "ax.grid()\n",
    "ax.legend()\n",
    "ax.set_title('State Value Estimates')\n",
    "ax.set_xlabel('Monte-Carlo')\n",
    "ax.set_ylabel('Agent')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\hat V_{Monte-Carlo}(s_t) = \\sum_{\\tau=0}^{episode~end} \\gamma^{\\tau-t}r_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there a big bias? It's ok, anyway it works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.** Plot several (say 3) states with high and low spreads of Q estimate by actions i.e.\n",
    "$$\\max_a \\hat Q(s,a) - \\min_a \\hat Q(s,a)\\$$\n",
    "Please take those states from different episodes to make sure that the states are really different.\n",
    "\n",
    "What should high and low spread mean at least in the world of perfect Q-fucntions?\n",
    "\n",
    "Comment the states you like most.\n",
    "\n",
    "**2.** Plot several (say 3) states with high td-error and several states with high values of\n",
    "$$| \\hat V_{Monte-Carlo}(s) - \\hat V_{agent}(s)|,$$ \n",
    "$$\\hat V_{agent}(s)=\\max_a \\hat Q(s,a).$$ Please take those states from different episodes to make sure that the states are really different. From what part (i.e. beginning, middle, end) of an episode did these states come from?\n",
    "\n",
    "Comment the states you like most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import play_and_log_episode, img_by_obs\n",
    "\n",
    "<YOUR CODE>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus area\n",
    "# Apply modifications to DQN (up to 10 points)\n",
    "\n",
    "For inspiration see [Rainbow](https://arxiv.org/abs/1710.02298) - a version of q-learning that combines lots of them.\n",
    "\n",
    "* [Quantile regression q-learning](https://arxiv.org/abs/1710.10044) (3 pts)\n",
    "* [Prioritized experience replay](https://arxiv.org/abs/1511.05952) (2 pts, use prioritized_sampler.py for template; +2 pts for bias correction)\n",
    "* [Double q-learning](https://arxiv.org/abs/1509.06461) (2 pt)\n",
    "* [Dueling q-learning](https://arxiv.org/abs/1511.06581) (2 pt)\n",
    "* [Noisy networks](https://arxiv.org/abs/1706.10295) (3 pts)\n",
    "* Multi-step heuristics (see [Rainbow](https://arxiv.org/abs/1710.02298)) (3 pts)\n",
    "* Something else? (depending on complexity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
